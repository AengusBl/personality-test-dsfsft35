{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfff3acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91b0e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding = \"this is a placeholder for the text embedding that will come later\"\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(len(text_embedding), 8)\n",
    "        self.fc2 = nn.Linear(8, 16)\n",
    "        self.fc3 = nn.Linear(16, 5) # Five OCEAN scores out\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x) # No activation function here: We want to use the sigmoid built into the loss func during training.\n",
    "        return x\n",
    "\n",
    "model = Model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceabd2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64637dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, epochs=100):\n",
    "    \"\"\"\n",
    "    Function to train a PyTorch model with training and validation datasets.\n",
    "\n",
    "    Parameters:\n",
    "    model: The neural network model to train.\n",
    "    train_loader: DataLoader for the training dataset.\n",
    "    val_loader: DataLoader for the validation dataset.\n",
    "    criterion: Loss function (e.g., Binary Cross Entropy for classification).\n",
    "    optimizer: Optimization algorithm (e.g., Adam, SGD).\n",
    "    epochs: Number of training epochs (default=100).\n",
    "\n",
    "    Returns:\n",
    "    history: Dictionary containing loss and accuracy for both training and validation.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct = 0, 0 \n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad() \n",
    "            outputs = model(inputs).squeeze() # TODO: Is the \"squeeze\" needed here?\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (torch.argmax(outputs,dim=1) == labels).sum().item()\n",
    "\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct = 0, 0\n",
    "        with torch.no_grad(): \n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs).squeeze() # TODO: Is the \"squeeze\" needed here?\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                val_correct += (torch.argmax(outputs,dim=1) == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / len(val_loader.dataset)\n",
    "\n",
    "        history['loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['accuracy'].append(train_acc)\n",
    "        history['val_accuracy'].append(val_acc)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    return history  # Return training history\n",
    "\n",
    "# Train the model\n",
    "history = train(model, train_loader, val_loader, criterion, optimizer, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df919cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our criterion nn.BCEWithLogitsLoss() has a sigmoid function built in, so we left out the sigmoid in the model.\n",
    "# This means we need to manually run the output through a sigmoid function when making predictions on a \"finished\" model.\n",
    "\n",
    "user_input = \"this is a placeholder for the data input by the user when the model is deployed\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(user_input)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    predictions = (probs > 0.5).int()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
